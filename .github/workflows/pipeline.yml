name: Tourism Project Pipeline

on:
  workflow_dispatch:
  push:
    branches: [ main ]

env:
  PYTHONUNBUFFERED: "1"

jobs:
  register-dataset:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: "Advanced Machine Learning and MLOps/Project"
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
      - name: Install Dependencies
        run: pip install -r ci-requirements.txt
      - name: Upload Dataset to Hugging Face Hub
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python - <<'PY'
          import os
          from pathlib import Path
          from huggingface_hub import HfApi, HfFolder, create_repo, upload_file

          token = os.environ.get("HF_TOKEN")
          assert token, "HF_TOKEN secret is required"
          HfFolder.save_token(token)
          api = HfApi()
          who = api.whoami(token=token)
          username = who.get('name') or (who.get('orgs') or [None])[0]
          assert username, "Cannot resolve Hugging Face username"

          dataset_repo = f"{username}/tourism-dataset"
          create_repo(dataset_repo, repo_type="dataset", exist_ok=True, token=token)

          local_csv = Path("tourism_project/data/tourism.csv")
          assert local_csv.exists(), f"Missing dataset: {local_csv}"

          upload_file(
              path_or_fileobj=str(local_csv),
              path_in_repo="data/tourism.csv",
              repo_id=dataset_repo,
              repo_type="dataset",
              token=token,
          )
          print(f"Dataset uploaded to https://huggingface.co/datasets/{dataset_repo}")
          PY

  data-prep:
    runs-on: ubuntu-latest
    needs: register-dataset
    defaults:
      run:
        working-directory: "Advanced Machine Learning and MLOps/Project"
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
      - name: Install Dependencies
        run: pip install -r ci-requirements.txt
      - name: Run Data Preparation
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python - <<'PY'
          import os
          import pandas as pd
          from pathlib import Path
          from sklearn.model_selection import train_test_split
          from huggingface_hub import HfApi, hf_hub_download, upload_file

          token = os.environ["HF_TOKEN"]
          api = HfApi()
          who = api.whoami(token=token)
          username = who.get('name') or (who.get('orgs') or [None])[0]
          dataset_repo = f"{username}/tourism-dataset"

          csv_path = hf_hub_download(
              repo_id=dataset_repo, repo_type="dataset", filename="data/tourism.csv", token=token
          )
          df = pd.read_csv(csv_path)
          df.columns = [c.strip() for c in df.columns]
          df = df.loc[:, ~df.columns.duplicated()].copy()
          if "CustomerID" in df.columns: df = df.drop(columns=["CustomerID"])
          df = df.drop_duplicates()
          miss = df.isna().mean()
          hi_na = miss[miss > 0.95].index.tolist()
          if hi_na: df = df.drop(columns=hi_na)
          nunique = df.nunique(dropna=True)
          const = [c for c in nunique[nunique <= 1].index if c != "ProdTaken"]
          if const: df = df.drop(columns=const)
          assert "ProdTaken" in df.columns, "ProdTaken missing"
          df["ProdTaken"] = df["ProdTaken"].astype(int)

          X, y = df.drop(columns=["ProdTaken"]), df["ProdTaken"]
          X_train, X_test, y_train, y_test = train_test_split(
              X, y, test_size=0.2, stratify=y, random_state=42
          )

          outdir = Path("tourism_project/data")
          outdir.mkdir(parents=True, exist_ok=True)
          train_local = outdir / "train.csv"
          test_local = outdir / "test.csv"
          pd.concat([X_train, y_train], axis=1).to_csv(train_local, index=False)
          pd.concat([X_test, y_test], axis=1).to_csv(test_local, index=False)

          upload_file(str(train_local), "data/train.csv", dataset_repo, "dataset", token)
          upload_file(str(test_local), "data/test.csv", dataset_repo, "dataset", token)
          print("Data prepared and uploaded.")
          PY

  model-traning:
    runs-on: ubuntu-latest
    needs: data-prep
    defaults:
      run:
        working-directory: "Advanced Machine Learning and MLOps/Project"
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
      - name: Install Dependencies
        run: pip install -r ci-requirements.txt
      - name: Start MLflow Server
        run: |
          nohup mlflow ui --host 0.0.0.0 --port 5000 >/dev/null 2>&1 &
          sleep 3
      - name: Model Building and Registration
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python - <<'PY'
          import os, json
          from pathlib import Path
          import joblib
          import pandas as pd
          from sklearn.compose import ColumnTransformer
          from sklearn.preprocessing import OneHotEncoder, StandardScaler
          from sklearn.impute import SimpleImputer
          from sklearn.pipeline import Pipeline
          from sklearn.ensemble import RandomForestClassifier
          from sklearn import metrics as skm
          from huggingface_hub import HfApi, hf_hub_download, create_repo, upload_file

          token = os.environ["HF_TOKEN"]
          api = HfApi()
          who = api.whoami(token=token)
          username = who.get('name') or (who.get('orgs') or [None])[0]
          dataset_repo = f"{username}/tourism-dataset"
          model_repo = f"{username}/tourism-purchase-predictor-rf"

          # Load train/test
          try:
              train_path = hf_hub_download(dataset_repo, "data/train.csv", token=token, repo_type="dataset")
              test_path  = hf_hub_download(dataset_repo, "data/test.csv",  token=token, repo_type="dataset")
          except Exception:
              train_path = "tourism_project/data/train.csv"
              test_path  = "tourism_project/data/test.csv"

          train_df = pd.read_csv(train_path)
          test_df  = pd.read_csv(test_path)

          y_train = train_df["ProdTaken"].astype(int); X_train = train_df.drop(columns=["ProdTaken"])
          y_test  = test_df["ProdTaken"].astype(int);  X_test  = test_df.drop(columns=["ProdTaken"])

          cat = X_train.select_dtypes(include=["object","category"]).columns.tolist()
          num = [c for c in X_train.columns if c not in cat]

          num_pipe = Pipeline([("imputer", SimpleImputer(strategy="median")), ("scaler", StandardScaler())])
          try:
              cat_pipe = Pipeline([("imputer", SimpleImputer(strategy="most_frequent")), ("ohe", OneHotEncoder(handle_unknown="ignore", sparse_output=False))])
          except TypeError:
              cat_pipe = Pipeline([("imputer", SimpleImputer(strategy="most_frequent")), ("ohe", OneHotEncoder(handle_unknown="ignore", sparse=False))])

          prep = ColumnTransformer([("num", num_pipe, num), ("cat", cat_pipe, cat)])
          clf = RandomForestClassifier(n_estimators=400, max_depth=None, random_state=42, class_weight="balanced")
          model = Pipeline([("prep", prep), ("clf", clf)])
          model.fit(X_train, y_train)

          y_pred = model.predict(X_test)
          y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None
          metrics = {
              "accuracy": float(skm.accuracy_score(y_test, y_pred)),
              "precision": float(skm.precision_score(y_test, y_pred, zero_division=0)),
              "recall": float(skm.recall_score(y_test, y_pred, zero_division=0)),
              "f1": float(skm.f1_score(y_test, y_pred, zero_division=0)),
              "roc_auc": float(skm.roc_auc_score(y_test, y_prob)) if y_prob is not None else None,
          }

          out = Path("tourism_project/model_building/models"); out.mkdir(parents=True, exist_ok=True)
          mpath = out / "best_model.joblib"; joblib.dump(model, mpath)
          with (out / "metrics.json").open("w") as f: json.dump(metrics, f, indent=2)

          create_repo(model_repo, repo_type="model", exist_ok=True, token=token)
          upload_file(str(mpath), "best_model.joblib", model_repo, "model", token)
          upload_file(str(out / "metrics.json"), "metrics.json", model_repo, "model", token)
          print(f"Model uploaded to https://huggingface.co/{model_repo}")
          PY

  deploy-hosting:
    runs-on: ubuntu-latest
    needs: [model-traning, data-prep, register-dataset]
    defaults:
      run:
        working-directory: "Advanced Machine Learning and MLOps/Project"
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
      - name: Install Dependencies
        run: pip install -r ci-requirements.txt
      - name: Push files to Hugging Face Space
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python - <<'PY'
          import os
          from pathlib import Path
          from huggingface_hub import HfApi, HfFolder, create_repo, upload_file

          token = os.environ["HF_TOKEN"]
          HfFolder.save_token(token)
          api = HfApi()
          who = api.whoami(token=token)
          username = who.get('name') or (who.get('orgs') or [None])[0]
          space_repo = f"{username}/tourism-predictor-app"

          # Ensure Space exists (idempotent)
          try:
              create_repo(space_repo, repo_type="space", exist_ok=True, token=token, space_sdk="streamlit")
          except TypeError:
              try:
                  api.repo_info(space_repo, repo_type="space", token=token)
              except Exception:
                  raise SystemExit("Create the Space once at https://huggingface.co/new-space (SDK=streamlit) and rerun.")

          assert Path("app.py").exists(), "Missing app.py"
          assert Path("requirements.txt").exists(), "Missing requirements.txt"

          Path("README.md").write_text(
              "---\n"
              "title: Tourism Purchase Predictor\n"
              "emoji: 🧭\n"
              "colorFrom: green\n"
              "colorTo: blue\n"
              "sdk: streamlit\n"
              "pinned: false\n"
              "app_file: app.py\n"
              "---\n",
              encoding="utf-8",
          )

          upload_file("app.py", "app.py", space_repo, "space", token)
          upload_file("requirements.txt", "requirements.txt", space_repo, "space", token)
          upload_file("README.md", "README.md", space_repo, "space", token)
          print(f"Space updated: https://huggingface.co/spaces/{space_repo}")
          PY