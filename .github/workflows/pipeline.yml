name: Tourism Project Pipeline

on:
  push:
    branches:
      - main
  workflow_dispatch:

permissions:
  contents: write

env:
  PYTHON_VERSION: "3.10"
  DATASET_LOCAL_PATH: "tourism_project/data/tourism.csv"
  DATASET_REPO_NAME: "tourism-dataset"
  MODEL_REPO_NAME: "tourism-purchase-predictor-rf"
  SPACE_NAME: "tourism-predictor-app"

jobs:
  register-dataset:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Print repo tree (root)
        run: |
          echo "Runner PWD: $(pwd)"
          ls -la
          echo "--- top-level dirs ---"
          find . -maxdepth 2 -type d -print | sed 's/^/  /'
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install "huggingface_hub>=0.24,<0.26" "datasets>=2.14,<3" pandas
      - name: Upload dataset to Hugging Face Hub
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python - <<'PY'
          import os, sys
          from huggingface_hub import HfApi, HfFolder, create_repo, upload_file
          from pathlib import Path
          token = os.environ.get("HF_TOKEN")
          if not token:
            print("Missing HF_TOKEN secret", file=sys.stderr); sys.exit(1)
          HfFolder.save_token(token)
          api = HfApi()
          who = api.whoami(token=token)
          username = who.get("name") or (who.get("orgs") or [None])[0]
          assert username, "Could not determine HF username from token."
          repo_id = f"{username}/{os.environ['DATASET_REPO_NAME']}"
          local_path = Path(os.environ["DATASET_LOCAL_PATH"])
          if not local_path.exists():
              print(f"Dataset file not found at {local_path}. Ensure it is committed.", file=sys.stderr)
              sys.exit(1)
          create_repo(repo_id, repo_type="dataset", exist_ok=True, token=token)
          upload_file(path_or_fileobj=str(local_path), path_in_repo="data/tourism.csv", repo_id=repo_id, repo_type="dataset", token=token)
          print(f"Dataset uploaded to https://huggingface.co/datasets/{repo_id}")
          PY

  data-prep:
    needs: register-dataset
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Print repo tree (root)
        run: |
          echo "Runner PWD: $(pwd)"
          ls -la
          echo "--- repo-root expected dirs ---"
          ls -la .
          echo "--- tourism_project dir listing ---"
          ls -la "tourism_project" || true
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas scikit-learn "huggingface_hub>=0.24,<0.26"
      - name: Prepare data and upload splits
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          DATASET_REPO_NAME: ${{ env.DATASET_REPO_NAME }}
        run: |
          python - <<'PY'
          import os, sys
          import pandas as pd
          from sklearn.model_selection import train_test_split
          from huggingface_hub import hf_hub_download, HfApi, upload_file
          from pathlib import Path
          token=os.environ.get("HF_TOKEN")
          api=HfApi()
          who=api.whoami(token=token)
          username=who.get("name") or (who.get("orgs") or [None])[0]
          repo_id=f"{username}/{os.environ['DATASET_REPO_NAME']}"
          csv_path = hf_hub_download(repo_id=repo_id, repo_type="dataset", filename="data/tourism.csv", token=token)
          df = pd.read_csv(csv_path)
          df.columns=[c.strip() for c in df.columns]
          if "CustomerID" in df.columns: df=df.drop(columns=["CustomerID"])
          df=df.loc[:, ~df.columns.duplicated()].copy()
          df=df.drop_duplicates()
          miss=df.isna().mean()
          df=df.drop(columns=miss[miss>0.95].index.tolist())
          nunique=df.nunique(dropna=True)
          drop_constant=[c for c in nunique[nunique<=1].index.tolist() if c!="ProdTaken"]
          if drop_constant: df=df.drop(columns=drop_constant)
          assert "ProdTaken" in df.columns, "ProdTaken not found"
          df["ProdTaken"]=df["ProdTaken"].astype(int)
          X=df.drop(columns=["ProdTaken"]); y=df["ProdTaken"]
          X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)
          train_df=pd.concat([X_train,y_train],axis=1)
          test_df=pd.concat([X_test,y_test],axis=1)
          out_dir=Path("tourism_project/data"); out_dir.mkdir(parents=True, exist_ok=True)
          train_local=out_dir/"train.csv"; test_local=out_dir/"test.csv"
          train_df.to_csv(train_local, index=False); test_df.to_csv(test_local, index=False)
          upload_file(path_or_fileobj=str(train_local), path_in_repo="data/train.csv", repo_id=repo_id, repo_type="dataset", token=token)
          upload_file(path_or_fileobj=str(test_local), path_in_repo="data/test.csv", repo_id=repo_id, repo_type="dataset", token=token)
          print("Prepared and uploaded splits.")
          PY
      - name: Commit prepared data artifacts back to main
        if: always()
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add tourism_project/data/train.csv tourism_project/data/test.csv || true
          git commit -m "CI: update prepared data splits [skip ci]" || echo "No changes to commit"
          git fetch origin main
          git pull --rebase origin main || true
          git push origin HEAD:main || echo "Push skipped (non-fast-forward)"

  model-training:
    needs: data-prep
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Print repo tree (root)
        run: |
          echo "Runner PWD: $(pwd)"
          ls -la
          echo "--- repo-root expected dirs ---"
          ls -la .
          echo "--- tourism_project dir listing ---"
          ls -la "tourism_project" || true
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install "numpy==1.26.4" "scipy==1.11.4" "scikit-learn==1.3.2" pandas joblib "huggingface_hub>=0.24,<0.26"
      - name: Train model and register to Hub
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          DATASET_REPO_NAME: ${{ env.DATASET_REPO_NAME }}
          MODEL_REPO_NAME: ${{ env.MODEL_REPO_NAME }}
        run: |
          python - <<'PY'
          import os, json
          import pandas as pd
          from pathlib import Path
          import joblib
          from sklearn.compose import ColumnTransformer
          from sklearn.preprocessing import OneHotEncoder, StandardScaler
          from sklearn.impute import SimpleImputer
          from sklearn.pipeline import Pipeline
          from sklearn.model_selection import GridSearchCV, StratifiedKFold
          from sklearn.ensemble import RandomForestClassifier
          from sklearn import metrics as skm
          from huggingface_hub import hf_hub_download, HfApi, HfFolder, create_repo, upload_file
          token=os.environ["HF_TOKEN"]
          api=HfApi(); HfFolder.save_token(token)
          who=api.whoami(token=token)
          username=who.get("name") or (who.get("orgs") or [None])[0]
          dataset_repo=f"{username}/{os.environ['DATASET_REPO_NAME']}"
          model_repo=f"{username}/{os.environ['MODEL_REPO_NAME']}"
          try:
              train_path=hf_hub_download(repo_id=dataset_repo, repo_type="dataset", filename="data/train.csv", token=token)
              test_path=hf_hub_download(repo_id=dataset_repo, repo_type="dataset", filename="data/test.csv", token=token)
          except Exception as e:
              raise SystemExit(f"Could not download splits: {e}")
          train_df=pd.read_csv(train_path); test_df=pd.read_csv(test_path)
          X_train=train_df.drop(columns=["ProdTaken"]); y_train=train_df["ProdTaken"].astype(int)
          X_test=test_df.drop(columns=["ProdTaken"]); y_test=test_df["ProdTaken"].astype(int)
          cat_cols=X_train.select_dtypes(include=["object","category"]).columns.tolist()
          num_cols=[c for c in X_train.columns if c not in cat_cols]
          numeric=Pipeline([( "imputer", SimpleImputer(strategy="median")), ("scaler", StandardScaler())])
          try:
              categorical=Pipeline([( "imputer", SimpleImputer(strategy="most_frequent")), ("ohe", OneHotEncoder(handle_unknown="ignore", sparse_output=False))])
          except TypeError:
              categorical=Pipeline([( "imputer", SimpleImputer(strategy="most_frequent")), ("ohe", OneHotEncoder(handle_unknown="ignore", sparse=False))])
          preprocess=ColumnTransformer([( "num", numeric, num_cols), ("cat", categorical, cat_cols)])
          pipe=Pipeline([( "prep", preprocess), ("clf", RandomForestClassifier(random_state=42, class_weight="balanced"))])
          param_grid={
              "clf__n_estimators":[200,400],
              "clf__max_depth":[None,5,10],
              "clf__min_samples_split":[2,5],
              "clf__min_samples_leaf":[1,2],
              "clf__max_features":["sqrt","log2",0.5],
          }
          cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
          grid=GridSearchCV(pipe, param_grid, cv=cv, scoring={"roc_auc":"roc_auc","f1":"f1","accuracy":"accuracy"}, refit="roc_auc", n_jobs=-1, verbose=1)
          grid.fit(X_train, y_train)
          best=grid.best_estimator_
          y_pred=best.predict(X_test)
          y_prob=best.predict_proba(X_test)[:,1]
          metrics={
              "accuracy": float(skm.accuracy_score(y_test,y_pred)),
              "precision": float(skm.precision_score(y_test,y_pred, zero_division=0)),
              "recall": float(skm.recall_score(y_test,y_pred, zero_division=0)),
              "f1": float(skm.f1_score(y_test,y_pred, zero_division=0)),
              "roc_auc": float(skm.roc_auc_score(y_test,y_prob)),
          }
          art_dir=Path("tourism_project/model_building/models"); art_dir.mkdir(parents=True, exist_ok=True)
          model_path=art_dir/"best_model.joblib"; joblib.dump(best, model_path)
          with (art_dir/"metrics.json").open("w") as f: json.dump(metrics,f,indent=2)
          # Upload artifacts
          create_repo(model_repo, repo_type="model", exist_ok=True, token=token)
          upload_file(path_or_fileobj=str(model_path), path_in_repo="best_model.joblib", repo_id=model_repo, repo_type="model", token=token)
          upload_file(path_or_fileobj=str(art_dir/"metrics.json"), path_in_repo="metrics.json", repo_id=model_repo, repo_type="model", token=token)
          print(f"Model registered at https://huggingface.co/{model_repo}")
          PY
      - name: Commit model artifacts back to main
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add tourism_project/model_building/models/* || true
          git commit -m "CI: add/update model artifacts [skip ci]" || echo "No changes to commit"
          git fetch origin main
          git pull --rebase origin main || true
          git push origin HEAD:main || echo "Push skipped (non-fast-forward)"

  deploy-hosting:
    needs: model-training
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Print repo tree (root)
        run: |
          echo "Runner PWD: $(pwd)"
          ls -la
          echo "--- repo-root expected files ---"
          ls -la .
          echo "--- tourism_project dir listing ---"
          ls -la "tourism_project" || true
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install "huggingface_hub>=0.24,<0.26"
      - name: Push files to Hugging Face Space
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python - <<'PY'
          import os
          from pathlib import Path
          from huggingface_hub import HfApi, HfFolder, upload_file
          token=os.environ["HF_TOKEN"]
          api=HfApi(); HfFolder.save_token(token)
          who=api.whoami(token=token)
          username=who.get("name") or (who.get("orgs") or [None])[0]
          space_repo=f"{username}/{os.environ.get('SPACE_NAME','tourism-predictor-app')}"
          root=Path(".")
          # ensure app files exist
          for p in [root/"app.py", root/"requirements.txt"]:
              if not p.exists():
                  raise SystemExit(f"{p} not found in repo")
          # ensure README contains valid Spaces front-matter at the top
          readme=root/"README.md"
          front_matter = (
              "---\n"
              "title: Tourism Purchase Predictor\n"
              "emoji: 🧭\n"
              "colorFrom: green\n"
              "colorTo: blue\n"
              "sdk: streamlit\n"
              "app_file: app.py\n"
              "pinned: false\n"
              "---\n"
          )
          existing = readme.read_text(encoding="utf-8") if readme.exists() else ""
          if existing.startswith("---"):
              lines = existing.splitlines()
              end_idx = None
              for i, line in enumerate(lines[1:], start=1):
                  if line.strip() == "---":
                      end_idx = i
                      break
              body = "\n".join(lines[end_idx+1:]) if end_idx is not None else ""
              new_readme = front_matter + ("\n\n" + body if body else "\n")
          else:
              new_readme = front_matter + ("\n\n" + existing if existing else "\n")
          readme.write_text(new_readme, encoding="utf-8")
          upload_file(path_or_fileobj=str(root/"app.py"), path_in_repo="app.py", repo_id=space_repo, repo_type="space", token=token)
          upload_file(path_or_fileobj=str(root/"requirements.txt"), path_in_repo="requirements.txt", repo_id=space_repo, repo_type="space", token=token)
          upload_file(path_or_fileobj=str(readme), path_in_repo="README.md", repo_id=space_repo, repo_type="space", token=token)
          print(f"Space updated: https://huggingface.co/spaces/{space_repo}")
          PY
